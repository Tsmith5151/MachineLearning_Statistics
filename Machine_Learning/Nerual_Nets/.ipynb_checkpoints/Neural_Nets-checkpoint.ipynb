{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural Networks Notes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "import os\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- The structure of the above nerual Network has 2 inputs and 1 output. The output will be denoted as yhat (i.e. the estimate, not ground truth). Any layer between our input and output layer is called a hidden layer. This neural network has one hidden layer with 3 hidden units. In the above structure, the circles represent neurons and lines represent synapses. \n",
    "\n",
    "**Synapses** \n",
    "\n",
    "- Take a value from their input, multiply it by a specific weight, and output the results.\n",
    "\n",
    "**Neurons** \n",
    "\n",
    "- Adds together the outputs of all their synapses, and apply an activation function.\n",
    "\n",
    "- Certain activation functions allow neural nets to model complex non-linear patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The input data will be a 3x2 matrix, and the output will be a 3x1 matrix. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Hours Study</th>\n",
       "      <th>Hours Sleep</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>10</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Hours Study  Hours Sleep\n",
       "0            3            5\n",
       "1            5            1\n",
       "2           10            2"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X = np.array([[3,5],[5,1],[10,2]])\n",
    "pd.DataFrame(X,columns=['Hours Study','Hours Sleep'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Output\n",
    "y = np.array([0.35,1.35,2.35])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each input value, or element in matrix X, needs to be multiplied by a corresponding weight and then added together with all the other results for each neuron. \n",
    "\n",
    "Here we have the input matrix, 3x2 multiplied by a 2x3 matrix of randomly initialized weights. This produces a 3x3 matrx,one row for each example, and one column for each hidden unit. $z^{(2)}$ refers to the second layer.\n",
    "\n",
    "Equation 1: $z^{(2)} = XW^{(1)}$\n",
    "\n",
    "Note: each entry in z is a sum of weighted inputs to each hidden neuron. Z is of size 3 by 3, one row for each example, and one column for each hidden unit.\n",
    "\n",
    "\n",
    "-Now that we have the activities for our second layer, z two, we need to apply the activation function. \n",
    "\n",
    "We'll independently apply the function to each entry in matrix z using a python method for this called sigmoid, because we’re using a sigmoid as our activation function. This is still a 3x3 matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAgUAAAF4CAYAAADEyIqPAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nACUFIiIiElFSICIiIoCSAhEREYn8P1ZUdxyQVk8wAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x112f49be0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def test_sigmoid():\n",
    "    \"\"\" Example of Sigmoid Function \"\"\"\n",
    "    x = np.arange(-6,6,0.01)\n",
    "    input  = sigmoid(x)\n",
    "    plt.plot(x,input)\n",
    "    plt.title('Sigmoid Activation Function',fontsize=12,y=1.02)\n",
    "test_sigmoid()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The second formula for forward propagation, is below. Using f to denote our activation function, we can write that $a^{(2)}$, our second layer activity, is equal to f of $z^{(2)}$. This is labeled $a^{(2)}$ in the Nerual Network code in the following block. \n",
    "\n",
    "Equation 2:   $a^{(2)} = f{(z^{(2)})}$\n",
    "\n",
    "To finish forward propagation we need to propagate $a^{(2)}$ all the way to the output, yhat. \n",
    "\n",
    "All we have to do now is multiply $a^{(2)}$ by our second layer weights W2 and apply one more activation funcion. \n",
    "\n",
    "W2 will be of size 3x1, one weight for each synapse. \n",
    "\n",
    "Multiplying $a^{(2)}$, a 3x3, by W2, we get a 3x1 matrix the activity for the third layer. $z^{(3)}$ has three activity values, one for each example. \n",
    "\n",
    "Finally, we'll apply our activation function to $z^{(2)}$ yielding the estimate of your test score, yHat.\n",
    "\n",
    "Equation 3:  $yhat = f{(z^{(3)})}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Implementing Forward Propagation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Test Score</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.613204</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.500052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.500000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Test Score\n",
       "0    0.613204\n",
       "1    0.500052\n",
       "2    0.500000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "N = Neural_Network()\n",
    "yhat = N.forward(X)\n",
    "pd.DataFrame(yHat,columns=['Test Score'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As we can tell below, the output has a relatively large errorand the model doesn't predict well. This is where the cost function comes into play. The idea be to find the combination of weights that minimize the error. \n",
    "\n",
    "Equation 4:  $Cost = \\frac{1}{2}*\\sum_(y-yhat)^2$\n",
    "\n",
    "We have a collection of 9, W1 which is a 2x3 matrix, and W2, a 3x1 matrix, and we're saying that there is some combination of w's that will make our cost, J, as small as possible. Keep in mind the idea behind the curse of dimensionality. If we were trying to minimize the cost for one of the 1 Weights and tried 1000 different values, it wouldn't take that long. Let's next consider 2 weights, to maintain the same precision we now need to check 1000 times 1000, or one million values. To do all 9 weights, well, that will be virutually impossible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Gradient Descent:\n",
    "\n",
    "We have 5 equations, but we can really think of them as one big equation.\n",
    "\n",
    "\n",
    "$Cost = \\frac{1}{2}*\\sum_(y-f(f(XW^{(1)})W^{(2)})^2$\n",
    "\n",
    "\n",
    "We want to know \"which way is downhill\", that is, what is the rate of change of J with respect to W, also known as the derivative. And in this case, since we’re just considering one weight at a time, the partial derivative -- $\\frac{dJ}{dW}$. \n",
    "\n",
    "\n",
    "\n",
    "We can derive an expression for $\\frac{dJ}{dW}$, that will give us the rate of change of J with respect to W, for any value of W! If $\\frac{dJ}{dW}$is positive, then the cost function is going uphill. If $\\frac{dJ}{dW}$ is negative the cost function is going downhill.\n",
    "\n",
    "\n",
    "To speed things up. Since we know in which direction the cost decreases, we can save all that time we would have spent searching in the wrong direction. We can save even more computational time by iteratively taking steps downhill and stopping when the cost stops getting smaller.\n",
    "\n",
    "\n",
    "This method is known as **gradient descent**, and although it may not seem so impressive in one dimension, it is capable of incredible speedups in higher dimensions. \n",
    "\n",
    "What if our cost function doesn't always go in the same direction? What if it goes up, then back down? The mathematical name for this is non-convex, and it could really throw off our gradient descent algorithm by getting it stuck in a local minima instead of our ideal global minima. One of the reasons we chose our cost function to be the sum of squared errors was to exploit the convex nature of quadratic equations.\n",
    "\n",
    "We know that the graph of y equals x squared is a nice convex parabola and it turns out that higher dimensional versions are too!\n",
    "\n",
    "If we use our examples one at a time instead of all at once, sometimes it won't matter if our cost function is convex, we will still find a good solution. This is called stochastic gradient descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we can take the weights W1 and W2 adn separate our $\\frac{dJ}{dW}$ computation in the same way, by computing $\\frac{dJ}{dW^{(2)}}$ and $\\frac{dJ}{dW^{(2)}}$ independently. \n",
    "\n",
    "\n",
    "Thus looking at $\\frac{dJ}{dW^{(2)}}$ first:\n",
    "\n",
    "$\\frac{dJ}{dW^{(2)}} = \\frac{1}{2}*\\sum(y-yhat)^{2}*{\\frac{d}{dW^{(2)}}}$\n",
    "\n",
    "We can move the $\\sum$ outside and just worry about the derivative of the inside expression first.\n",
    "\n",
    "$\\frac{dJ}{dW^{(2)}} = \\sum\\frac{1}{2}*(y-yhat)^{2}*{\\frac{d}{dW^{(2)}}}$\n",
    "\n",
    "Now we apply the chain rule, where we take the derivative of the outside function and then multiply it by the derivative of the inside function. Note: y is just our test scores, which won’t change, so the derivative of y, a constant, with respect to $dW^{(2)}$ is zero. \n",
    "\n",
    "$\\frac{dJ}{dW^{(2)}} = (y-yhat)^{2}*{\\frac{d}{dW^{(2)}}}$\n",
    "\n",
    "The question now is how do we take derivative of yhat with respect to $dW^{(2)}$. Recall, that yhat is our activation function of z3, so we can break down as $\\frac{dyhat}{dz^{(3)}}$ and  $\\frac{dz}{dW^{(2)}}$\n",
    "\n",
    "\n",
    "To find the rate of change of yHat with respect to z3, we need to differentiate our sigmoid activation function with respect to z.\n",
    "\n",
    "f(z) = $\\frac{1}{(1+e^{-z})^2}$\n",
    "\n",
    "f'(z) = $\\frac{e^{-z}}{(1+e^{-z})^2}$\n",
    "\n",
    "\n",
    "Next, we can replace $\\frac{dyhat}{dz^{(3)}}$ with $f'(z^{(3)})$ and our equation now becomes:\n",
    "\n",
    "$\\frac{dJ}{dW^{(2)}} = (y-yhat)*f`(z^{(3)})*{\\frac{d}{dW^{(2)}}}$\n",
    "\n",
    "\n",
    "Keep in mind that for each synapse, dz/dW(2), this is just the activation, a on that synapse:\n",
    "\n",
    "$z^{(3)} = a^{(2)}*W^{(2)}\n",
    "\n",
    "Another way to think about what the calculus is doing here is that it is “backpropagating” the error to each weight, by multiplying by the activity on each synapses, the weights that contribute more to the error will have larger activations, and yield larger dJ/dW2 values, and those weights will be changed more when we perform gradient descent.\n",
    "\n",
    "\n",
    "Note: $f'(z^{(3)}$ is of the same size, 3x1 matrix. This 3x1 matrix is referred to as the backpropagating error, delta 3.\n",
    "\n",
    "Next,$\\frac{dz^{(3)}}/{dW^{(2)}}$ is equal to the activity of each synapse. Each value in delta 3 needs to be multiplied by each activity. We can achieve this by transposing a2 and matrix multiplying by delta3.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class Neural_Network(object):\n",
    "    def __init__(self):\n",
    "        self.inputLayerSize = 2\n",
    "        self.outputLayerSize = 1\n",
    "        self.hiddenLayerSize = 3\n",
    "        \n",
    "        #Initialize Random Weights\n",
    "        self.W1 = np.random.randn(self.inputLayerSize, \\\n",
    "                                   self.hiddenLayerSize)\n",
    "        self.W2 = np.random.randn(self.hiddenLayerSize,\\\n",
    "                                   self.outputLayerSize)\n",
    "    \n",
    "    def forward(self,X):\n",
    "        #W1 = 2x3 matrix, X = 3x2 matrix\n",
    "        self.z2 = np.dot(X,self.W1)\n",
    "        #a2 = 3x3 matrix\n",
    "        self.a2 =  self.sigmoid(self.z2)\n",
    "        #a2 = 3x3 matrix, W2 = 3x1 matrix\n",
    "        self.z3 = np.dot(self.a2,self.W2)\n",
    "        #3x1 matrix\n",
    "        yhat = self.sigmoid(self.z3)\n",
    "        return(yhat)\n",
    "   \n",
    "    def sigmoid(self,z):\n",
    "        \"\"\" Compute Sigmoid Activation Function\"\"\"\n",
    "        sig = 1/(1 + np.exp(-z))\n",
    "        return(sig)\n",
    "    \n",
    "    def sigmoid_prime(z):\n",
    "        \"\"\" Compute Derivative of Sigmoid Activation Function\"\"\"\n",
    "        f_prime = np.exp(-z) / (1+np.exp(-z))**2\n",
    "        return(f_prime)\n",
    "    \n",
    "    def cost_function_prime(self,X,y):\n",
    "        #Compute derivative with respect to W and W2 for a given X and y:\n",
    "        self.yhat = self.forward(X)\n",
    "        \n",
    "        delta3 = np.multiply(-(y-self.yhat),self.sigmoidPrime(self.z3))\n",
    "        dJdW2 = np.dot(self.a2.T,delta3)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda env:ML]",
   "language": "python",
   "name": "conda-env-ML-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
